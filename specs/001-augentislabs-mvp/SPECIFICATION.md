# Feature Specification: AugentisLabs MVP Platform

**Feature Branch**: `001-augentislabs-mvp`  
**Created**: 2025-11-22  
**Status**: Draft  
**Input**: Build complete AI-powered Software Factory MVP with 5D methodology, 26 agents, and quality gates

---

## User Scenarios & Testing _(mandatory)_

### User Story 1 - Founder Discovers & Validates Business Idea (Priority: P1) ğŸ¯ MVP

**Description**: As a technical founder with a product idea, I want to systematically validate whether my idea is worth building by conducting market research, competitive analysis, and venture readiness assessment so that I can make data-driven decisions before investing time and money.

**Why this priority**: This is the critical entry point. Every founder using AugentisLabs starts here. VRC assessment (Discover phase) prevents wasted effort on non-viable ideas, directly supporting the platform's core value proposition of reducing venture failure rates from 90% to <70%.

**Independent Test**: Can be fully tested by:

1. Submitting a raw 2-3 sentence business idea
2. Receiving automated problem validation, competitive analysis, and market sizing
3. Getting a VRC (Venture Readiness Compass) score â‰¥76% (pass decision) with actionable remediation if needed
4. Proceeding to Define phase OR receiving specific guidance on what research is needed to improve VRC score

Delivers: Data-driven validation confidence, clear go/no-go decision, evidence-based next steps.

**Acceptance Scenarios**:

1. **Given** founder submits idea "AI-powered time tracking for remote teams", **When** system processes through Discover agents, **Then** system returns: Problem clarity assessment (E1-E2 evidence tier), competitive landscape (3-5 competitors with feature matrix), market opportunity (TAM/SAM/SOM), and VRC score calculation across 20 indicators

2. **Given** VRC score is â‰¥76%, **When** founder reviews results, **Then** "Proceed to Define" button is enabled with clear rationale

3. **Given** VRC score is 50-75% (CONDITIONAL), **When** founder views results, **Then** system shows: Which of 20 indicators are below threshold + specific remediation guidance (e.g., "Conduct 3 more user interviews on persona 2")

4. **Given** VRC score is <50% (FAIL), **When** founder reviews, **Then** system clearly states idea is not viable with this problem statement, recommends pivot direction or market repositioning

5. **Given** founder requests detailed evidence for any indicator, **When** they click indicator, **Then** system shows: Original assessment methodology, data sources used (interviews, market research), confidence level (E0-E4), and raw evidence collected

---

### User Story 2 - Founder Defines Market Positioning & Product Requirements (Priority: P1)

**Description**: As a founder whose idea passed VRC â‰¥76%, I want to define my market positioning, create detailed customer personas, and generate a comprehensive product requirements document so that my development team has clear, validated specifications and I can validate product-market fit assumptions before engineering effort.

**Why this priority**: Define phase unlocks product development. It transforms raw market insights (Discover) into executable requirements and validated business strategy. VCD gate (â‰¥75%) ensures we proceed with well-founded strategy, not guesses.

**Independent Test**: Can be fully tested by:

1. Uploading market research (interviews, surveys, analyst reports)
2. Receiving 3-5 customer personas with JTBD narratives, decision triggers, and segment-specific pain points
3. Getting comprehensive PRD with 50-100 prioritized user stories (MVP vs Phase 2), acceptance criteria, and business model validation
4. Obtaining VCD score â‰¥75% confirming market research quality and strategic readiness
5. Exporting ready-for-design artifacts (personas, feature prioritization matrix, pricing recommendations)

Delivers: Validated product strategy, clear feature prioritization (MVP vs Phase 2), founder confidence before engineering investment.

**Acceptance Scenarios**:

1. **Given** founder uploads 15 customer interview transcripts, **When** Interview Ingestion Agent processes, **Then** system extracts: Key pain points (quoted), desired outcomes (JTBD format), decision-making criteria, objection patterns

2. **Given** market research artifacts are processed, **When** Persona Builder completes, **Then** system generates: 3-5 detailed personas (demographics, goals, pain points, JTBD narratives, buying triggers) with evidence tier per persona attribute

3. **Given** personas and pain points are defined, **When** Requirements Generator runs, **Then** system creates: PRD with 50-100 user stories, feature prioritization (MVP vs Phase 2), acceptance criteria per story, business model assumptions, pricing recommendations

4. **Given** all Define phase agents complete, **When** VCD Assembler scores readiness, **Then** system calculates: VCD score across 10 indicators (market research quality, persona validation, feature prioritization, business model, competitive positioning) and gate decision (PASS â‰¥75% / CONDITIONAL 50-75% / FAIL <50%)

5. **Given** VCD â‰¥75%, **When** founder clicks "Export to Design", **Then** system packages: PRD (frozen), Personas (with evidence), Feature Matrix (prioritized), Competitive Positioning, Pricing Strategy, and Design Brief for UX/Architecture teams

---

### User Story 3 - Designer Creates UI/UX & System Architecture (Priority: P1)

**Description**: As a UX/product designer, I want to receive validated product requirements and personas, then generate comprehensive wireframes, user journey maps, design system components, and system architecture diagrams so that developers have complete specifications and can start coding without design rework.

**Why this priority**: Design phase gates prevent developers from building wrong product. DSP assessment (â‰¥80%) ensures wireframes cover â‰¥80% of user stories, API spec is complete, security/accessibility are designed in (not bolted on), and architecture is appropriate for scale/compliance requirements.

**Independent Test**: Can be fully tested by:

1. Importing PRD and personas from Define phase
2. Receiving 80%+ wireframes covering all user stories, interactive prototype, and design system (50+ components)
3. Getting C4 architecture diagram, OpenAPI 3.0 spec, database ERD, and deployment architecture
4. Obtaining DSP score â‰¥80% confirming design completeness
5. Exporting design assets and API specifications to developers

Delivers: Complete design specifications, developer-ready API contracts, no ambiguity for implementation.

**Acceptance Scenarios**:

1. **Given** PRD with 50 user stories imported, **When** UX Agent generates wireframes, **Then** system creates: â‰¥40 wireframes (80% coverage), user flows, interaction specifications, accessibility considerations (WCAG 2.1 AA minimum)

2. **Given** personas and PRD, **When** Journey Map Agent creates maps, **Then** system generates: 3-5 end-to-end journey maps per persona, emotion curves, pain point markers, opportunity indicators, touchpoint details

3. **Given** user stories and journeys, **When** Architecture Agent designs system, **Then** system produces: C4 architecture (Context/Container/Component/Code levels), OpenAPI 3.0 spec (all endpoints, request/response schemas, error codes), database ERD, deployment topology

4. **Given** design system requirements, **When** Branding Agent completes, **Then** system generates: Design system library (Figma file), 50+ components, color palette, typography, spacing system, accessibility checklist

5. **Given** all Design phase outputs complete, **When** DSP Assembler scores, **Then** system calculates: DSP across 8 components (wireframe coverage â‰¥80%, design system complete, API spec complete, accessibility design â‰¥AA, security design present, test strategy defined, performance targets set, deployment strategy clear) and gate decision (PASS â‰¥80%)

---

### User Story 4 - Developer Generates & Deploys Production Code (Priority: P1)

**Description**: As a full-stack developer, I want to receive complete design specifications and API contracts, then generate production-ready code with tests, security scanning, and performance optimization so that I can deploy a working MVP in weeks instead of months.

**Why this priority**: Develop phase is where code is generated. MDP gate (â‰¥85%) ensures tests pass, security vulnerabilities are fixed, performance targets are met, and compliance checklist is complete before Deploy phase.

**Independent Test**: Can be fully tested by:

1. Importing API spec, wireframes, and database schema from Design phase
2. Receiving scaffolded backend (NestJS), frontend (Next.js), database schema (Prisma), test stubs (Jest/Supertest)
3. Getting 70-80% complete implementation with TDD tests written first (failing initially)
4. Running security scan and receiving vulnerability remediation
5. Obtaining MDP score â‰¥85% confirming production readiness
6. Deploying to staging environment (Vercel + Supabase)

Delivers: Fully functional MVP ready for beta users, test-first discipline embedded, security validated, performance optimized.

**Acceptance Scenarios**:

1. **Given** OpenAPI spec and wireframes imported, **When** Code Generator runs, **Then** system generates: Backend scaffold (NestJS with dependency injection), endpoint stubs for all API operations, request/response validation, error handling structure

2. **Given** database schema from Architecture, **When** Schema Generator runs, **Then** system creates: Prisma schema (all models, relationships, constraints), migration files, indexes, database seeders for test data

3. **Given** user stories and acceptance criteria, **When** Test Generator runs, **Then** system creates: Jest test files with failing tests (RED phase), Supertest integration tests, contract tests validating OpenAPI compliance, test fixtures and mocks

4. **Given** wireframes and design system, **When** Frontend Generator runs, **Then** system creates: Next.js components (scaffolded), React hooks structure, routing, API client integration stubs, state management setup

5. **Given** code scaffold complete, **When** Security Scanner runs, **Then** system produces: Security audit report (vulnerabilities categorized by CVSS severity), remediation code patches, security checklist (OWASP Top 10, data protection, secrets management)

6. **Given** all code generated, **When** MDP Assembler scores, **Then** system calculates: MDP across 6 components (test coverage â‰¥80%, code review checklist, performance profiling <300ms p95, security scan passed, compliance ready, cost forecast within budget) and gate decision (PASS â‰¥85%)

---

### User Story 5 - Founder Launches Product & Monitors Divergence (Priority: P2)

**Description**: As a founder with a production-ready MVP, I want to deploy my application to production with real-time monitoring, collect user telemetry, and receive automated alerts when actual usage diverges from Define-phase assumptions so that I can respond quickly with targeted fixes instead of full redesigns.

**Why this priority**: Deploy phase enables continuous feedback loops. Divergence detection (when actual user behavior â‰  Define assumptions) triggers selective re-execution (update only affected Design screens + Code modules), reducing fix time from weeks to days.

**Independent Test**: Can be fully tested by:

1. Clicking "Deploy to Production" â†’ automatic deployment to Vercel + Supabase
2. Receiving live telemetry dashboards (feature usage, user cohorts, retention curves, crash rates, P95 latency)
3. Getting weekly divergence reports comparing actual telemetry to Define assumptions
4. Seeing automatic alerts if divergence >2 evidence tiers (e.g., "Persona age assumption 35 but actual 42, pricing assumption $15 but 60% churning at $10 competitor")
5. Approving proposed updates from Define-Update Proposer, triggering Cascade analyzer to determine minimum re-execution scope

Delivers: Live product with real user data, early detection of product-market misfit, rapid iteration capability.

**Acceptance Scenarios**:

1. **Given** MDP â‰¥85% and founder approves deployment, **When** Deployment Agent runs, **Then** system: Packages code for Vercel, provisions Supabase PostgreSQL instance, configures environment variables, deploys frontend + backend, runs smoke tests, sets up monitoring dashboards

2. **Given** application live with users, **When** Telemetry Aggregator collects data, **Then** system captures: Feature usage (which screens used daily), user cohorts (segment analysis), retention curves (Day 1, Day 7, Day 30), crash rates, API latency distribution (P50/P95/P99)

3. **Given** 1 week of production telemetry collected, **When** Divergence Detector runs, **Then** system compares: Define assumptions (persona age 35, pricing $15/mo, usage 2 hrs/day) vs actual telemetry (users age 42 avg, 60% churn at $10 competitor, usage 1.2 hrs/day), classifies divergences (MAJOR >50% delta, MINOR 20-50%, COSMETIC <20%)

4. **Given** MAJOR divergence detected (persona age +7 years), **When** Define-Update Proposer generates proposal, **Then** system shows: Specific persona v2 changes (age 35â†’42, adjusted goals/pain points), PRD updates affected (3 user stories), and confidence score (E4 production-proven vs E1 original assumption)

5. **Given** founder approves Define v2, **When** Cascade Impact Analyzer calculates scope, **Then** system determines: Which Design screens affected (3 screens, 2 journeys), which Code modules affected (1 validation module, 1 UI component), effort estimate (3 days vs 3 weeks for full redesign)

---

### User Story 6 - Product Manager Measures Success & Governs AI Decisions (Priority: P2)

**Description**: As a product manager/CTO, I want to see all AI agent decisions, confidence levels, and cost impacts, with human approval gates for strategic decisions so that I maintain control over product strategy and LLM spend doesn't spiral while still getting AI acceleration benefits.

**Why this priority**: Governance ensures AI doesn't make expensive mistakes autonomously. Quality/Cost/Safety/Strategic gates with human approval authority maintain human control over critical decisions while AI handles routine tasks.

**Independent Test**: Can be fully tested by:

1. Accessing decision audit trail showing every AI agent output with confidence scores (E0-E4)
2. Approving/rejecting quality gates (VRC, VCD, DSP, MDP) with override authority
3. Setting cost thresholds and seeing alerts when LLM usage approaches budget
4. Approving strategic decisions (target ICP changes, pricing pivots, platform architecture changes)
5. Viewing compliance checklist for regulatory requirements (SOC 2, GDPR, data retention)

Delivers: Full transparency into AI decisions, cost control, strategic oversight, compliance assurance.

**Acceptance Scenarios**:

1. **Given** VRC assessment complete, **When** PO views quality gate summary, **Then** system shows: 20-indicator scores with evidence tiers, lowest-scoring indicators with remediation guidance, gate decision (PASS/CONDITIONAL/FAIL) with override authority

2. **Given** Define phase completion, **When** PO accesses cost dashboard, **Then** system displays: LLM costs (Discover: $120, Define: $340, total YTD: $2,100), forecast (on track for $3,500 monthly), alerts if forecast exceeds $5,000 threshold

3. **Given** MAJOR divergence detected, **When** PO reviews divergence report, **Then** system shows: Divergence classification (MAJOR), Define assumptions vs actual telemetry, confidence scores (E4 for telemetry, E1 for original assumptions), Define-Update Proposer recommendations with confidence scores

4. **Given** founder requests Target ICP pivot (from solo founders to small agencies), **When** Strategic Gate triggers, **Then** system: Requests PO/CTO approval with rationale, shows impact analysis (3 months delay, $200K additional LLM cost), updates sales/marketing strategy if approved

5. **Given** compliance checklist phase, **When** PO reviews requirements (SOC 2, GDPR, HIPAA), **Then** system displays: Checklist status (80% complete), missing items (Data encryption at rest, DPA signatures), automated compliance reporting for audits

---

## Requirements _(mandatory)_

### Functional Requirements

**Discover Phase (4 Agents)**:

- **FR-001**: System MUST accept raw 2-3 sentence business idea and extract problem statement, target market, and value proposition hypothesis
- **FR-002**: System MUST conduct automated competitive research (web scraping, database queries) and return top 5-10 competitors with feature matrix, pricing, market share estimates
- **FR-003**: System MUST search public market signals (Reddit, Twitter, Google Trends, Product Hunt) and classify demand validation evidence tier (E0-E4)
- **FR-004**: System MUST calculate TAM/SAM/SOM estimates using market sizing methodologies with E1-E2 evidence tier classification
- **FR-005**: System MUST assess venture readiness across 20 indicators (Problem, Opportunity, Evidence, Readiness) and produce VRC score (0-100%) with gate decision (PASS â‰¥76% / CONDITIONAL 50-75% / FAIL <50%)

**Define Phase (5 Agents)**:

- **FR-006**: System MUST ingest market research documents (PDFs, transcripts, survey data) and extract structured insights (market size, trends, regulatory environment)
- **FR-007**: System MUST process customer interview transcripts and generate 3-5 personas with demographics, goals, pain points, JTBD narratives, and buying triggers
- **FR-008**: System MUST generate 50-100 prioritized user stories from personas and market research with acceptance criteria (BDD Given-When-Then format) and evidence tier
- **FR-009**: System MUST assess business model feasibility and produce VCD score (0-100%) across 10 indicators with gate decision (PASS â‰¥75%)
- **FR-010**: System MUST export frozen artifacts (PRD v1, Personas, Feature Matrix, Competitive Positioning) as handoff to Design phase

**Design Phase (6 Agents)**:

- **FR-011**: System MUST generate wireframes for â‰¥80% of user stories with interaction flows and accessibility annotations (WCAG 2.1 AA minimum)
- **FR-012**: System MUST create design system library (50+ components) with colors, typography, spacing, icon set, and component documentation
- **FR-013**: System MUST produce C4 architecture diagrams (Context/Container/Component levels), OpenAPI 3.0 specification (all endpoints), and database ERD
- **FR-014**: System MUST generate end-to-end user journey maps (3-5 per persona) with emotion curves and pain point markers
- **FR-015**: System MUST produce DSP score (0-100%) across 8 components (wireframe coverage, design system, API spec, accessibility, security design, test strategy, performance targets, deployment strategy) with gate decision (PASS â‰¥80%)

**Develop Phase (7 Agents)**:

- **FR-016**: System MUST generate backend scaffold (NestJS) with endpoint stubs for OpenAPI spec, request/response validation, error handling structure
- **FR-017**: System MUST generate database schema (Prisma) with models, relationships, constraints, indexes, and migration files
- **FR-018**: System MUST generate frontend components (Next.js) scaffolding, routing, API client integration, state management hooks
- **FR-019**: System MUST generate test files (Jest unit tests, Supertest integration tests, contract tests) with FAILING tests initially (RED-Green-Refactor TDD)
- **FR-020**: System MUST execute security scanning (SAST, dependency scanning, container scanning) and return vulnerability report with remediation code patches
- **FR-021**: System MUST profile code performance and return optimization recommendations (caching, indexing, batch processing) to achieve <300ms p95 latency target
- **FR-022**: System MUST produce MDP score (0-100%) across 6 components (test coverage â‰¥80%, code review checklist, security scan passed, performance targets met, compliance ready, cost forecast) with gate decision (PASS â‰¥85%)

**Deploy Phase (4 Agents)**:

- **FR-023**: System MUST deploy application to production (Vercel frontend, Supabase backend) with zero-downtime deployment and rollback capability
- **FR-024**: System MUST configure monitoring dashboards (Vercel Analytics, Supabase Dashboard, custom telemetry) collecting feature usage, user cohorts, retention curves, crash rates, P95 latency
- **FR-025**: System MUST detect divergence (Define assumptions vs Deploy telemetry) and classify as MAJOR (>50% delta), MINOR (20-50%), or COSMETIC (<20%)
- **FR-026**: System MUST propose Define v2 updates with specific persona/PRD changes and confidence scores when divergence detected
- **FR-027**: System MUST analyze cascade impact (which Design screens, Code modules affected) and estimate re-execution effort vs full redesign

**Multi-Tenant & Governance**:

- **FR-028**: System MUST enforce multi-tenant isolation (each org's data encrypted, RLS policies, audit logs)
- **FR-029**: System MUST implement 4 quality gate types (Quality, Safety, Cost, Strategic) with human approval authority and override capability
- **FR-030**: System MUST track LLM costs per customer and enforce budget thresholds with alerts and automatic fallback to human review
- **FR-031**: System MUST maintain evidence tier classification (E0-E4) for all artifacts and enforce minimum thresholds at phase gates
- **FR-032**: System MUST provide full decision audit trail with AI confidence scores, cost impacts, and approval history

### Key Entities

- **Venture**: Customer's business idea, phases completed (Discover/Define/Design/Develop/Deploy), gate scores (VRC/VCD/DSP/MDP)
- **Persona**: Target customer profile with demographics, goals, pain points, JTBD narratives, decision triggers, evidence tier per attribute
- **UserStory**: Prioritized feature (P1/P2/P3), acceptance criteria (BDD format), requirement mappings, test references, code modules
- **Artifact**: Output from agent (PRD, persona, wireframe, code, test), versioning (v1/v2/v3), evidence tier, traceability links (upstream requirements, downstream code)
- **Divergence**: Mismatch between Define assumption (E1-E2) and Deploy telemetry (E4), classification (MAJOR/MINOR/COSMETIC), impact analysis
- **QualityGate**: VRC/VCD/DSP/MDP assessment, score calculation, gate decision logic, approval authority, override history
- **LLMCost**: API call tracking (model, tokens, cost), per-customer monthly budget, cost forecasting, alerts

### Edge Cases

- What happens when founder submits idea with <100 character description? â†’ System prompts for more detail before running Discover agents
- How does system handle when no competitors found in market research? â†’ Flags as RED signal (market too new or problem not real), adjusts VRC scoring
- What if customer interviews contradict market reports? â†’ System flags as CONFLICT, increases evidence tier requirement (need E3+ to resolve), shows both sides to founder
- How does system handle when API spec and design wireframes contradict? â†’ Design phase blocks progression, Design team must reconcile before DSP gate can pass
- What if production telemetry shows zero users after Week 1 deployment? â†’ System alerts deployment issue or no marketing, recommends go/no-go review before defining divergence-triggered updates
- What if founder tries to skip a phase (go directly from Define to Develop)? â†’ System blocks progression, explains gate requirements, requires CTO override with written justification
- What if LLM API (OpenAI/Anthropic) is down during code generation? â†’ System queues request, retries with exponential backoff, escalates to human developer if >4 hours down
- What if user story has acceptance criteria that conflicts with budget constraints? â†’ Cost Gate alerts founder, shows trade-off (feature vs budget), requires strategic decision

## Success Criteria _(mandatory)_

### Measurable Outcomes

**Founder Experience**:

- **SC-001**: Founder can submit idea to MVP deployment in â‰¤16 weeks (vs 6-12 months typical), tracked via gate completion timestamps
- **SC-002**: Founder receives VRC decision within 24 hours of idea submission (Discover phase completes in <1 day)
- **SC-003**: 80% of founders with VRC â‰¥76% report increased confidence in product strategy, measured via NPS survey at Define phase completion
- **SC-004**: Generated PRD covers â‰¥80% of founder's intended feature set (measured via founder PRD review/approval), reducing specification rework by 60%
- **SC-005**: Generated code initially failing TDD tests (RED phase) then passing within 3 weeks (GREEN phase complete), demonstrating test-first discipline

**Platform Reliability**:

- **SC-006**: Platform uptime â‰¥99.5% (monthly), monitored via Vercel + Supabase dashboards
- **SC-007**: API response latency <300ms P95 (Discover, Define, Design agent API calls), measured via CloudFlare Analytics
- **SC-008**: Agent execution success rate â‰¥95% (Code Generator, Test Generator, etc. successfully produce usable output first try), tracked via execution logs

**Quality & Compliance**:

- **SC-009**: Generated applications achieve >95% Lighthouse performance scores (Speed, Accessibility, Best Practices, SEO), validated via Lighthouse CI on every deployment
- **SC-010**: Security scan finds zero critical vulnerabilities in generated code (CVSS 9-10), measured via SAST results
- **SC-011**: Generated test suites achieve â‰¥80% code coverage (unit + integration tests), measured via Jest coverage reports

**Business Metrics**:

- **SC-012**: Customer acquisition: 50 customers (all tiers) by Month 6, 200 customers by Month 12 (measured via Stripe recurring revenue)
- **SC-013**: Retention: >35% Day 30 retention, >60% Day 90 retention (customers completing Define phase), measured via product analytics
- **SC-014**: NPS: â‰¥50 NPS score from customers who completed Define phase (MVP), measured via Typeform survey
- **SC-015**: LLM cost efficiency: Generate complete MVP application for <$5,000 total LLM cost per customer, tracked via OpenAI/Anthropic API logs
- **SC-016**: Revenue: $50K MRR Month 6, $250K MRR Month 12, $1M+ MRR Month 24

**Venture Success Rate** (Long-term):

- **SC-017**: Customers' ventures achieve product-market fit (retention â‰¥40% Month 3) at >30% rate (vs 10% industry baseline), measured at Month 12
- **SC-018**: Customers' generated applications launch to production within 16 weeks, measured via Deploy phase completion date
- **SC-019**: 60-80% cost reduction vs traditional development agencies for MVP development, calculated via customer survey + benchmarking

---

## Architecture & Technical Approach _(mandatory)_

### Platform Components

**1. Backend API (NestJS + Node.js)**

- Agent orchestration engine (LangGraph-based workflow engine)
- Quality gate calculation engine (VRC/VCD/DSP/MDP scoring)
- Multi-tenant isolation & RLS policies
- LLM API integrations (OpenAI GPT-4 Turbo, Anthropic Claude 3.5 Sonnet)
- Artifact versioning & traceability graph

**2. Frontend Web Application (Next.js + React)**

- Multi-step 5D phase workflow UI
- Decision audit trail & transparency dashboard
- Artifact view/download (PRD, personas, wireframes, code exports)
- Cost dashboard & budget monitoring
- Real-time telemetry dashboards (Deploy phase)

**3. Database (PostgreSQL via Supabase)**

- Multi-tenant schema with Row-Level Security (RLS)
- Artifact storage (versioned PRDs, personas, wireframes, generated code)
- Telemetry data (feature usage, user cohorts, crash logs)
- Audit logs (decisions, approvals, LLM costs)

**4. Code Generation Engine**

- Template-driven code generation (Handlebars/EJS templates)
- OpenAPI spec parsing â†’ NestJS endpoint scaffolding
- Prisma schema generation from ERD
- Next.js component generation from wireframes
- Jest/Supertest test file generation from user stories

**5. AI Agent System (LangGraph Orchestration)**

- 26 specialized agents (4 Discover, 5 Define, 6 Design, 7 Develop, 4 Deploy)
- Workflow state persistence (retryable, resumable)
- Error handling & human escalation
- LLM prompt versioning & A/B testing

**6. Monitoring & Observability**

- Vercel Analytics (frontend performance)
- Supabase Dashboard (database metrics)
- Custom telemetry (feature usage, user cohorts, retention)
- OpenTelemetry integration for distributed tracing
- Sentry for error tracking

### Deployment Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Browser (Next.js Frontend on Vercel Edge)          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Next.js API Routes + NestJS Backend Microservices  â”‚
â”‚  (Vercel Serverless + Container Registry)           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Supabase (PostgreSQL + Auth + Real-time)           â”‚
â”‚  AWS S3 (Artifact storage + code exports)           â”‚
â”‚  OpenAI/Anthropic APIs (LLM orchestration)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Assumptions & Constraints

**Assumptions**:

- Founders will provide â‰¥5 customer interviews for Define phase (required for VCD gate)
- Market research documents (PDFs, transcripts) are text-extractable (OCR available)
- OpenAI/Anthropic APIs remain stable (99.5% uptime SLA assumed)
- Generated applications will be deployed to Vercel/Supabase (assumed standard infrastructure for MVP)

**Constraints**:

- Constitution freeze: No new languages/frameworks without written approval (stack: Node.js, TypeScript, NestJS, Next.js, PostgreSQL, LangGraph)
- LLM token limit: Average 50K tokens per venture through full 5D pipeline (cost cap ~$5K/venture)
- Test-first mandatory: All generated code must have failing tests first (RED-Green-Refactor discipline)
- Multi-tenant isolation: GDPR compliance required (data encryption, data residency, audit logs)
- Evidence tier minimum: No phase gate can pass below E2 evidence tier for critical requirements

---

## Appendix: 5D Phase Gates & Evidence Tiers

### Phase Gate Thresholds (Constitution Requirement)

| Phase Gate                | Threshold | Remediation                        | Override                    |
| ------------------------- | --------- | ---------------------------------- | --------------------------- |
| **VRC** (Discoverâ†’Define) | â‰¥76%      | Conduct additional interviews      | Product Owner only          |
| **VCD** (Defineâ†’Design)   | â‰¥75%      | Refine market research             | Product Owner only          |
| **DSP** (Designâ†’Develop)  | â‰¥80%      | Complete missing wireframes        | Product Owner + Design Lead |
| **MDP** (Developâ†’Deploy)  | â‰¥85%      | Fix failing tests, security issues | CTO only                    |

### Evidence Tier Requirements

| Tier | Confidence | Min Requirement            | Example                                 |
| ---- | ---------- | -------------------------- | --------------------------------------- |
| E0   | 0-20%      | NOT ACCEPTABLE             | "We think..." (founder opinion)         |
| E1   | 20-40%     | Initial exploration only   | 2 interviews, 1 market report           |
| E2   | 40-70%     | Minimum for Define phase   | 10+ interviews, 3+ competitors analyzed |
| E3   | 70-85%     | Preferred for Design phase | Industry analyst validation             |
| E4   | 85-100%    | Deploy phase standard      | Production telemetry from â‰¥1K users     |
